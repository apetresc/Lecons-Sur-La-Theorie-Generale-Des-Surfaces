% !TEX root = ../master.tex
% !TEX encoding = UTF-8 Unicode

\Chapter{Sur L'Intégration Du Système Linéaire}{Qui Se Présente Dans La Théorie Précédente}
\label{chp2}

\Verse{13} Il nous reste à étudier d'une manière détaillée l'intégration du système
\begin{empheq}[left=\empheqlbrace]{equation}
\begin{aligned}
\frac{\d{\alpha}}{\d{t}} &= \beta r - \gamma q, \\
\frac{\d{\beta}}{\d{t}} &= \gamma p - \alpha r \\
\frac{\d{\gamma}}{\d{t}} &= \alpha q - \beta p,
\end{aligned} \label{eqn-2.1}
\end{empheq}
auquel satisfont les trois groupes de cosinus. Nous avons déjà signalé une propriété fondamentale de ce système. Il 
admet l'intégrale quadratique
\begin{equation}
\begin{aligned}
\alpha^2 + \beta^2 + \gamma^2 = \textrm{const.,}
\end{aligned} \label{eqn-2.2}
\end{equation}
et l'existence de cette intégrale entraîne comme corollaires une série de propositions qui facilitent, dans plusieurs 
cas, l'intégration du système.

Avant de commencer l'étude des équations (\ref{eqn-2.1}), je vais montrer d'abord que tout système linéaire de la forme
\begin{empheq}[left=\empheqlbrace]{equation}
\begin{aligned}
\frac{\d{\alpha}}{\d{t}} &= A\alpha + B\beta + C\gamma, \\
\frac{\d{\beta}}{\d{t}} &= A'\alpha + B'\beta + C'\gamma, \\
\frac{\d{\gamma}}{\d{t}} &= A''\alpha + B''\beta + C''\gamma,
\end{aligned} \label{eqn-2.3}
\end{empheq}
où $A$, $B$, $C$, $A'$, \dots\, sont des fonctions de $t$, peut être ramené à la forme (\ref{eqn-2.1}) toutes les fois 
qu'il admet une intégrale du second degré connue
\begin{equation}
\varphi(\alpha, \beta, \gamma) = \textrm{const.},
\label{eqn-2.4}
\end{equation}
où $\varphi$ désigne une fonction homogène du second degré, à coefficients constants ou variables.

En effet, une substitution linéaire homogène quelconque ne change évidemment pas la forme des équations (\ref{eqn-2.3}) 
et, à l'aide d'une telle substitution, on peut évidemment ramener l'équation (\ref{eqn-2.4}) à la forme
\begin{equation}
\alpha^2 + \beta^2 + \gamma^2 = \textrm{const.,}
\label{eqn-2.5}
\end{equation}
en laissant de côté les cas exceptionnels, qu'on traitera aisément, où la fonction $\varphi$ serait un carré, ou une 
somme de carrés.

Si l'on exprime que le premier membre de l'équation (\ref{eqn-2.5}) est une intégrale du système (\ref{eqn-2.3}), on 
obtient les conditions
\[
A = B' = C'' = B + A' = C + A'' = C' + B'' = 0,
\]
qui permettent de ramener ce système (\ref{eqn-2.3}) à la forme (\ref{eqn-2.1}).

Ainsi, le système (\ref{eqn-2.1}) nous apparaît comme le type, ou la \textit{forme réduite} d'une classe entière de 
systèmes présentant la propriété, qu'on rencontre fréquemment dans les applications, d'admettre une intégrale connue du 
second degré. Ce caractère particulier des équations que nous allons étudier méritait d'être approfondi et suffirait à 
justifier l'étendue des développements qui vont suivre\footnote{On peut caractériser aussi le système étudié en disant 
qu'il est identique à son système \textit{adjoint}. Voir pour la définition du système adjoint deux \textit{Notes} de 
l'Auteur insérées en 1880 aux \textit{Comptes rendus}, t. 90, pp. 524 et 596.}.

\Verse{14} Je vais montrer d'abord que, toutes les fois que l'on connaîtra une solution particulière $(\alpha_0, 
\beta_0, \gamma_0)$ du système (\ref{eqn-2.1}), on pourra joindre l'intégrale du première degré
\[
\alpha\alpha_0 + \beta\beta_0 + \gamma\gamma_0 = \textrm{const.},
\]
à l'intégrale déjà donnée du second degré.

En effet, si l'on a une solution quelconque $(\alpha, \beta, \gamma)$ du système (\ref{eqn-2.1}), on en pourra déduire, 
d'après les propriétés de tout système linéaire, une solution plus générale
\[
\alpha + k\alpha_0, \quad \beta + k\beta_0, \quad \gamma + k\gamma_0,
\]
$k$ désignant une constante quelconque. On devra donc avoir, pour toutes les valeurs de $k$,
\[
(\alpha + k\alpha_0)^2 + (\beta + k\beta_0)^2 + (\gamma + k\gamma_0)^2 = \textrm{const.},
\]
ou, en développant,
\[
\alpha^2 + \beta^2 + \gamma^2 + 2k(\alpha\alpha_0 + \beta\beta_0 + \gamma\gamma_0) + k^2(\alpha_0^2 + \beta_0^2 + 
\gamma_0^2) = \textrm{const.}
\]

Le premier et le dernier terme du premier membre étant constants, il en sera de même de
\[
\alpha\alpha_0 + \beta\beta_0 + \gamma\gamma_0,
\]
comme il fallait de démontrer.

Il résulte évidemment de là que, si l'on connaissait seulement deux solutions particulières du système (\ref{eqn-2.1}), 
$(\alpha_0, \beta_0, \gamma_0)$, $(\alpha_1, \beta_1, \gamma_1)$, on pourrait immédiatement écrire la solution 
générale, qui serait définie par les équations
\begin{align*}
\alpha^2 + \beta^2 + \gamma^2 &= \textrm{const.}, \\
\alpha\alpha_0 + \beta\beta_0 + \gamma\gamma_0 &= \textrm{const.}, \\
\alpha\alpha_1 + \beta\beta_1 + \gamma\gamma_1 &= \textrm{const.};
\end{align*} 
ces équations peuvent être résolue et donnent pour $\alpha$, $\beta$, $\gamma$ les valeurs
\begin{empheq}[left=\empheqlbrace]{equation}
\begin{aligned}
\alpha &= c_0\alpha_0 + c_1\alpha_1 + c_2(\beta_0\gamma_1 - \beta_1\gamma_0), \\
\beta &= c_0\beta_0 + c_1\beta_1 + c_2(\gamma_0\alpha_1 - \alpha_0\gamma1), \\
\gamma &= c_0\gamma_0 + c_1\gamma_1 + c_2(\alpha_0\beta_1 - \alpha_1\beta_0),
\end{aligned} \label{eqn-2.6}
\end{empheq}
où $c_0$, $c_1$, $c_2$ désignent des constantes arbitraires. Mais on peut obtenir une proposition plus complète et 
montrer que, si l'on connaît une seule solution du système (\ref{eqn-2.1}), une seule quadrature suffira à nous donner 
son intégrale générale.

\Verse{15} Pour établir ce résultat essentiel, remarquons que les valeurs les plus générales de $\alpha$, $\beta$, 
$\gamma$ doivent satisfaire à la relation
\[
\alpha^2 + \beta^2 + \gamma^2 = \textrm{const.}
\]

Commençons d'abord par écarter le cas où la constante serrait nulle; on pourra toujours, en divisant ces valeurs par 
une constant convenable, supposer que l'on a
\begin{equation}
\alpha^2 + \beta^2 + \gamma^2 = 1.
\label{eqn-2.7}
\end{equation}

Remarquons même que, dans le problème particulier que nous avons à traiter, $\alpha$, $\beta$, $\gamma$ étant trois 
cosinus directeurs, doivent nécessairement satisfaire à cette relation. Il est naturel d'exprimer $\alpha$, $\beta$, 
$\gamma$ en fonction de deux variables indépendantes, de manière que la relation précédente soit toujours satisfaite, 
et de chercher ces deux variables.

Or, si l'on regarde $\alpha$, $\beta$, $\gamma$ comme les coordonnées d'un point de l'espace, l'équation 
(\ref{eqn-2.7}) représentera une sphère de rayon 1, ayant pour le centre l'origine des coordonnées. Considérons cette 
sphère comme une surface réglée, admettant un double système de génératrices imaginaires, et prenons pour variables 
deux quantités demeurant constantes respectivement sur les génératrices de chaque système. Pour cela, nous poserons
\begin{empheq}[left=\empheqlbrace]{equation}
	\begin{aligned}
		\frac{\alpha + i\beta}{1 - \gamma} &= \frac{1 + \gamma}{\alpha - i\beta} = x, \\
		\frac{\alpha - i\beta}{1 - \gamma} &= \frac{1 + \gamma}{\alpha + i\beta} = -\frac{1}{y},
	\end{aligned} \label{eqn-2.8}
\end{empheq}
ce qui donnera
\begin{equation}
	\alpha = \frac{1 - xy}{x - y}, \quad \beta = i\frac{1 + xy}{x - y}, \quad \gamma = \frac{x + y}{x - y}.
	\label{eqn-2.9}
\end{equation}

Remarquons que, d'après les formules (\ref{eqn-2.8}), $x$ et $y$ seront imaginaires quand $\alpha$, $\beta$, $\gamma$ 
seront réels, et, en outre, l'imaginaire conjuguée de $x$ sera $-\frac{1}{y}$.

Si nous substituons les valeurs (\ref{eqn-2.9}) de $\alpha$, $\beta$, $\gamma$ dans les équations différentielles, ces 
équations se réduiront à deux, comme on devait s'y attendre, et après quelques calcules faciles, on obtiendra le système
\begin{empheq}[left=\empheqlbrace]{equation}
	\begin{aligned}
		\frac{\d{x}}{\d{t}} &= -ir\,x + \frac{q - ip}{2} + \frac{q + ip}{2}x^2, \\
		\frac{\d{y}}{\d{t}} &= -ir\,y + \frac{q - ip}{2} + \frac{q + ip}{2}y^2;
	\end{aligned} \label{eqn-2.10}
\end{empheq}
$x$ et $y$ doivent être deux solutions différentes de la même équation en $\sigma$
\begin{equation}
	\frac{\d\sigma}{\d{t}} = -ir\,\sigma + \frac{q - ip}{2} + \frac{q + ip}{2}\sigma^2,
	\label{eqn-2.11}
\end{equation}
et l'intégration du système proposé est ramenée à celle de cette seule équation. Deux solutions particulières 
distinctes de cette équation donneront toujours, par l'emploi de cette (\ref{eqn-2.9}), des valeurs réelles ou 
imaginaires de $\alpha$, $\beta$, $\gamma$, vérifiant le système (\ref{eqn-2.1}). Remarquons même que, lorsque $p$, 
$q$, $r$ seront des fonctions réelles, il suffira de connaître une solution particulière $\sigma$ de l'équation 
(\ref{eqn-2.11}) pour en déduire une solution $(\alpha, \beta, \gamma)$ du système proposé. En effet, désignons par 
$\sigma'$ l'imaginaire conjuguée de $\sigma$. Je vais montrer que $-\frac{1}{\sigma'}$ est encore une solution 
particulière de l'équation (\ref{eqn-2.11}).

Pour cela changeons $i$ en $-i$ dans cette équation, nous aurons
\[
	\frac{\d{\sigma'}}{\d{t}} = + ir\sigma' + \frac{q+ ip}{2} + \frac{q - ip}{2}\sigma'^2
\]
et, par conséquent,
\[
	\frac{\d{}}{\d{t}}\left(-\frac{1}{\sigma'}\right) = -ir\left(-\frac{1}{\sigma'}\right) + \frac{q - ip}{2} + \frac{q 
	+ ip}{2}\left(-\frac{1}{\sigma'}\right)^2.
\]
Il suffit de comparer à l'équation (\ref{eqn-2.11}) pour reconnaître que $-\frac{1}{\sigma'}$ est bien une solution 
particulière de cette équation.

\Verse{16} L'équation en $\sigma$ appartient au groupe des équations de la forme
\begin{equation}
	\frac{\d{\sigma}}{\d{t}} = a + 2b\sigma + c\sigma^2,
	\label{eqn-2.13}
\end{equation}
où $a$, $b$, $c$ sont des fonctions quelconques de $t$. Ce sont les plus simples après les équations linéaires. Comme 
on les rencontre fréquemment dans les applications, on leur a donné le nom de Riccati, parce qu'elles comprennent comme 
cas particulier l'équation
\[
	\frac{\d{\sigma}}{\d{t}} = a\sigma^2 + bt^m,
\]
qui, seule, a été l'objet des recherches du géomètre italien. Nous allons rappeler rapidement leurs principales 
propriétés.

D'abord, elles ne changent pas de forme quand on effectue sur $\sigma$ une substitution linéaire, c'est-à-dire quand on 
substitue à $\sigma$ la variable $\lambda$ définie par l'équation
\[
	\lambda = \frac{P\sigma + Q}{R\sigma + S}
\]
où $P$, $Q$, $R$, $S$ sont des fonctions quelconques de $t$.

En second lieu, on peut les intégrer dès que l'on en connaît une solution particulière. Soit, en effet, 
$\sigma=\sigma_0$ une telle solution. Posons
\[
	\sigma = \sigma_0 + \frac{1}{\lambda},
\]
et nous obtiendrons pour $\lambda$ l'équation linéaire
\begin{equation}
	\frac{\d{\lambda}}{\d{t}} = -c -2(c\sigma_0 + b)\lambda,
	\label{eqn-2.14}
\end{equation}
dont l'intégration exigera seulement deux quadratures effectuées successivement.

De là résulte une des propriétés fondamentales de l'équation de Riccati. Comme la valeur générale de $\lambda$ est 
linéaire par rapport à la constante arbitraire $C$ et de la forme
\[
	PC + Q,
\]
on voit que l'intégrale générale de l'équation de Riccati sera de la forme
\[
	\sigma = \frac{RC + S}{PC + Q},
\]
$P$, $Q$, $R$, $S$ étant des fonctions de la variable indépendante de $t$. On déduit de là que \textit{le rapport 
anharmonique de quatre solutions de l'équation est constant et égal à celui des quatre valeurs de la constante 
arbitraire correspondantes à ces solutions}.